{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohuelab/ColabDesign-cyclic-binder/blob/cyc_binder/cyclic_peptide_binder_design.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is based on the AfDesign binder hallucination protocol in ColabDesign, which is published by Dr. Sergey Ovchinnikov on [GitHub](https://github.com/sokrypton/ColabDesign/tree/main/af)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OA2k3sAYuiXe"
      },
      "source": [
        "# AfDesign - cyclic peptide binder design\n",
        "For a given protein target and cyclic binder length, generate/hallucinate a cyclic binder sequence AlphaFold thinks will bind to the target structure. To do this, we maximize number of contacts at the interface and maximize pLDDT of the binder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-AXy0s_4cKaK"
      },
      "outputs": [],
      "source": [
        "#@title **setup**\n",
        "import os\n",
        "if not os.path.isdir(\"params\"):\n",
        "  # get code\n",
        "  os.system(\"pip -q install git+https://github.com/ohuelab/ColabDesign-cyclic-binder.git@cyc_binder\")\n",
        "  # for debugging\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign\")\n",
        "  # download params\n",
        "  os.system(\"mkdir params\")\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(\"aria2c -q -x 16 https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\")\n",
        "  os.system(\"tar -xf alphafold_params_2022-12-06.tar -C params\")\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import os\n",
        "from colabdesign import mk_afdesign_model, clear_mem\n",
        "from colabdesign.shared.utils import copy_dict\n",
        "from colabdesign.af.alphafold.common import residue_constants\n",
        "\n",
        "from IPython.display import HTML\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#########################\n",
        "def get_pdb(pdb_code=\"\"):\n",
        "  if pdb_code is None or pdb_code == \"\":\n",
        "    upload_dict = files.upload()\n",
        "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "    with open(\"tmp.pdb\",\"wb\") as out: out.write(pdb_string)\n",
        "    return \"tmp.pdb\"\n",
        "  elif os.path.isfile(pdb_code):\n",
        "    return pdb_code\n",
        "  elif len(pdb_code) == 4:\n",
        "    os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
        "    return f\"{pdb_code}.pdb\"\n",
        "  else:\n",
        "    os.system(f\"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v3.pdb\")\n",
        "    return f\"AF-{pdb_code}-F1-model_v3.pdb\"\n",
        "  \n",
        "def add_cyclic_offset(self, bug_fix=True):\n",
        "  '''add cyclic offset to connect N and C term'''\n",
        "  def cyclic_offset(L):\n",
        "    i = np.arange(L)\n",
        "    ij = np.stack([i,i+L],-1)\n",
        "    offset = i[:,None] - i[None,:]\n",
        "    c_offset = np.abs(ij[:,None,:,None] - ij[None,:,None,:]).min((2,3))\n",
        "    if bug_fix:\n",
        "      a = c_offset < np.abs(offset)\n",
        "      c_offset[a] = -c_offset[a]\n",
        "    return c_offset * np.sign(offset)\n",
        "  idx = self._inputs[\"residue_index\"]\n",
        "  offset = np.array(idx[:,None] - idx[None,:])\n",
        "\n",
        "  if self.protocol == \"binder\":\n",
        "    c_offset = cyclic_offset(self._binder_len)\n",
        "    offset[self._target_len:,self._target_len:] = c_offset\n",
        "  self._inputs[\"offset\"] = offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HSgE99WALOE-"
      },
      "outputs": [],
      "source": [
        "#@title **prep inputs**\n",
        "import re\n",
        "#@markdown ---\n",
        "#@markdown **target info**\n",
        "pdb = \"1YCR\" #@param {type:\"string\"}\n",
        "#@markdown - enter PDB code or UniProt code (to fetch AlphaFoldDB model) or leave blink to upload your own\n",
        "target_chain = \"A\" #@param {type:\"string\"}\n",
        "target_hotspot = \"\" #@param {type:\"string\"}\n",
        "if target_hotspot == \"\": target_hotspot = None\n",
        "#@markdown - restrict loss to predefined positions on target (eg. \"1-10,12,15\")\n",
        "target_flexible = False #@param {type:\"boolean\"}\n",
        "#@markdown - allow backbone of target structure to be flexible\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **binder info**\n",
        "cyclic_offset = True #@param {type:\"boolean\"}\n",
        "#@markdown - if True, use cyclic petide complex offset for hallucination of cyclic peptides\n",
        "bugfix = True #@param {type:\"boolean\"}\n",
        "#@markdown - if True, use bug fiexed version for cyclic offset\n",
        "binder_len = 13 #@param {type:\"integer\"}\n",
        "#@markdown - length of binder to hallucination\n",
        "binder_seq = \"\" #@param {type:\"string\"}\n",
        "binder_seq = re.sub(\"[^A-Z]\", \"\", binder_seq.upper())\n",
        "if len(binder_seq) > 0:\n",
        "  binder_len = len(binder_seq)\n",
        "else:\n",
        "  binder_seq = None\n",
        "#@markdown - if defined, will initialize design with this sequence\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **model config**\n",
        "use_multimer = False #@param {type:\"boolean\"}\n",
        "#@markdown - use alphafold-multimer for design\n",
        "num_recycles = 0 #@param [\"0\", \"1\", \"3\", \"6\"] {type:\"raw\"}\n",
        "num_models = \"2\" #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"all\"]\n",
        "num_models = 5 if num_models == \"all\" else int(num_models)\n",
        "#@markdown - number of trained models to use during optimization\n",
        "\n",
        "x = {\"pdb_filename\":pdb,\n",
        "     \"chain\":target_chain,\n",
        "     \"binder_len\":binder_len,\n",
        "     \"hotspot\":target_hotspot,\n",
        "     \"use_multimer\":use_multimer,\n",
        "     \"rm_target_seq\":target_flexible}\n",
        "     \n",
        "x[\"pdb_filename\"] = get_pdb(x[\"pdb_filename\"])     \n",
        "\n",
        "if \"x_prev\" not in dir() or x != x_prev:\n",
        "  clear_mem()\n",
        "  model = mk_afdesign_model(\n",
        "    protocol=\"binder\",\n",
        "    use_multimer=x[\"use_multimer\"],\n",
        "    num_recycles=num_recycles,\n",
        "    recycle_mode=\"sample\"\n",
        "  )\n",
        "  model.prep_inputs(\n",
        "    **x,\n",
        "    ignore_missing=False\n",
        "  )\n",
        "  x_prev = copy_dict(x)\n",
        "  print(\"target length:\", model._target_len)\n",
        "  print(\"binder length:\", model._binder_len)\n",
        "  binder_len = model._binder_len\n",
        "\n",
        "# Set cyclic offset\n",
        "if cyclic_offset:\n",
        "  if bugfix:\n",
        "      print(\"Set bug fixed cyclic peptide complex offset. The cyclic peptide binder will be hallucionated.\")\n",
        "      add_cyclic_offset(model, bug_fix=True)\n",
        "  else:\n",
        "      print(\"Set not bug fixed cyclic peptide complex offset. The cyclic peptide binder will be hallucionated.\")\n",
        "      add_cyclic_offset(model, bug_fix=False)\n",
        "else:\n",
        "  print(\"Don't set cyclic offset. The linear peptide binder will be hallucionated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title **cyclic peptide complex offset visualization**\n",
        "# cyclic peptide complex offset\n",
        "offset = model._inputs[\"offset\"]\n",
        "offset[model._target_len:, :model._target_len] = 0\n",
        "offset[:model._target_len, model._target_len:] = 0\n",
        "plt.figure()\n",
        "plt.title(\"cyclic peptide complex offset\")\n",
        "plt.imshow(offset, cmap=\"bwr_r\", vmin=-model._target_len-5, vmax=model._target_len+5)\n",
        "# cyclic peptide complex offset (cyclic peptide only)\n",
        "plt.figure()\n",
        "plt.title(\"cyclic peptide complex offset (cyclic peptide only)\")\n",
        "for i in range(model._binder_len):\n",
        "    for j in range(model._binder_len):\n",
        "        plt.text(j, i, str(offset[i][j]), va='center', ha='center', fontsize=8)\n",
        "plt.imshow(offset[-model._binder_len:, -model._binder_len:], cmap=\"bwr_r\", vmin=-model._binder_len-5, vmax=model._binder_len+5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "60qmxpzno0yV"
      },
      "outputs": [],
      "source": [
        "#@title **run AfDesign**\n",
        "from scipy.special import softmax\n",
        "\n",
        "optimizer = \"pssm_semigreedy\" #@param [\"pssm_semigreedy\", \"3stage\", \"semigreedy\", \"pssm\", \"logits\", \"soft\", \"hard\"]\n",
        "#@markdown - `pssm_semigreedy` - uses the designed PSSM to bias semigreedy opt. (Recommended)\n",
        "#@markdown - `3stage` - gradient based optimization (GD) (logits → soft → hard)\n",
        "#@markdown - `pssm` - GD optimize (logits → soft) to get a sequence profile (PSSM).\n",
        "#@markdown - `semigreedy` - tries X random mutations, accepts those that decrease loss\n",
        "#@markdown - `logits` - GD optimize logits inputs (continious)\n",
        "#@markdown - `soft` - GD optimize softmax(logits) inputs (probabilities)\n",
        "#@markdown - `hard` - GD optimize one_hot(logits) inputs (discrete)\n",
        "\n",
        "#@markdown WARNING: The output sequence from `pssm`,`logits`,`soft` is not one_hot. To get a valid sequence use the other optimizers, or redesign the output backbone with another protocol like ProteinMPNN.\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown #### advanced GD settings\n",
        "GD_method = \"sgd\" #@param [\"adabelief\", \"adafactor\", \"adagrad\", \"adam\", \"adamw\", \"fromage\", \"lamb\", \"lars\", \"noisy_sgd\", \"dpsgd\", \"radam\", \"rmsprop\", \"sgd\", \"sm3\", \"yogi\"]\n",
        "learning_rate = 0.1 #@param {type:\"raw\"}\n",
        "norm_seq_grad = True #@param {type:\"boolean\"}\n",
        "dropout = True #@param {type:\"boolean\"}\n",
        "\n",
        "model.restart(seq=binder_seq)\n",
        "model.set_optimizer(optimizer=GD_method,\n",
        "                    learning_rate=learning_rate,\n",
        "                    norm_seq_grad=norm_seq_grad)\n",
        "models = model._model_names[:num_models]\n",
        "\n",
        "flags = {\"num_recycles\":num_recycles,\n",
        "         \"models\":models,\n",
        "         \"dropout\":dropout}\n",
        "\n",
        "if optimizer == \"3stage\":\n",
        "  model.design_3stage(120, 60, 10, **flags)\n",
        "  pssm = softmax(model._tmp[\"seq_logits\"],-1)\n",
        "\n",
        "if optimizer == \"pssm_semigreedy\":\n",
        "  model.design_pssm_semigreedy(120, 32, **flags)\n",
        "  pssm = softmax(model._tmp[\"seq_logits\"],1)\n",
        "\n",
        "if optimizer == \"semigreedy\":\n",
        "  model.design_pssm_semigreedy(0, 32, **flags)\n",
        "  pssm = None\n",
        "\n",
        "if optimizer == \"pssm\":\n",
        "  model.design_logits(120, e_soft=1.0, num_models=1, ramp_recycles=True, **flags)\n",
        "  model.design_soft(32, num_models=1, **flags)\n",
        "  flags.update({\"dropout\":False,\"save_best\":True})\n",
        "  model.design_soft(10, num_models=num_models, **flags)\n",
        "  pssm = softmax(model.aux[\"seq\"][\"logits\"],-1)\n",
        "\n",
        "model.save_pdb(f\"{model.protocol}.pdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A1GxeLZdTTya"
      },
      "outputs": [],
      "source": [
        "#@title display hallucinated protein {run: \"auto\"}\n",
        "color = \"pLDDT\" #@param [\"chain\", \"pLDDT\", \"rainbow\"]\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "color_HP = False #@param {type:\"boolean\"}\n",
        "animate = True #@param {type:\"boolean\"}\n",
        "model.plot_pdb(show_sidechains=show_sidechains,\n",
        "               show_mainchains=show_mainchains,\n",
        "               color=color, color_HP=color_HP, animate=animate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2E9Tn2Acchj"
      },
      "outputs": [],
      "source": [
        "ani = model.animate(dpi=100)\n",
        "HTML(ani.to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSKWYu0_GlUH"
      },
      "outputs": [],
      "source": [
        "model.save_pdb(f\"{model.protocol}.pdb\")\n",
        "model.get_seqs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3pvptBM55GHU"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Amino acid probabilties\n",
        "import plotly.express as px\n",
        "alphabet = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "if \"pssm\" in dir() and pssm is not None:\n",
        "  fig = px.imshow(pssm.mean(0).T,\n",
        "                  labels=dict(x=\"positions\", y=\"amino acids\", color=\"probability\"),\n",
        "                  y=residue_constants.restypes,\n",
        "                  zmin=0,\n",
        "                  zmax=1,\n",
        "                  template=\"simple_white\",\n",
        "                )\n",
        "  fig.update_xaxes(side=\"top\")\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SGmdJKLNKvb"
      },
      "outputs": [],
      "source": [
        "# log\n",
        "model._tmp[\"best\"][\"aux\"][\"log\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
